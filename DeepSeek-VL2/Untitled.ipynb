{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee71dd9-e6b5-4ef9-96bb-52ca56e9fb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/quynd/vidCapLLM/DeepSeek-VL2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7723633b-f50d-4248-a932-013afdb154d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5009b2c3-6326-487e-af36-b1f3a94c9198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: requests in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (76.0.0)\n",
      "Requirement already satisfied: wheel in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.45.1)\n",
      "Requirement already satisfied: cmake in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.31.6)\n",
      "Requirement already satisfied: lit in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (18.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from requests->torchvision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from requests->torchvision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from requests->torchvision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from requests->torchvision) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from jinja2->torch==2.0.1->torchvision) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9b8186-1d25-4746-ba64-d5a0728d2351",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'attrdict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepseek_vl2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepseek_vl2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pil_images\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# specify the path to the model\u001b[39;00m\n",
      "File \u001b[0;32m~/vidCapLLM/DeepSeek-VL2/deepseek_vl2/models/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2023-2024 DeepSeek.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Permission is hereby granted, free of charge, to any person obtaining a copy of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_deepseek_vl_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekVLV2Processor\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_deepseek_vl_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepseekVLV2ForCausalLM\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepseekVLV2Processor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepseekVLV2ForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m ]\n",
      "File \u001b[0;32m~/vidCapLLM/DeepSeek-VL2/deepseek_vl2/models/modeling_deepseek_vl_v2.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mattrdict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttrDict\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'attrdict'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images\n",
    "\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/deepseek-vl2-tiny\"\n",
    "vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "## single image conversation example\n",
    "## Please note that <|ref|> and <|/ref|> are designed specifically for the object localization feature. These special tokens are not required for normal conversations.\n",
    "## If you would like to experience the grounded captioning functionality (responses that include both object localization and reasoning), you need to add the special token <|grounding|> at the beginning of the prompt. Examples could be found in Figure 9 of our paper.\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"<image>\\n<|ref|>The giraffe at the back.<|/ref|>.\",\n",
    "        \"images\": [\"./images/visual_grounding_1.jpeg\"],\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation,\n",
    "    images=pil_images,\n",
    "    force_batchify=True,\n",
    "    system_prompt=\"\"\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "# run image encoder to get the image embeddings\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "# run the model to get the response\n",
    "outputs = vl_gpt.language.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=False)\n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03047160-1e45-418b-9d07-6204c202d131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/quynd/myenv/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/quynd/myenv/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/quynd/myenv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/quynd/myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/quynd/myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/quynd/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/quynd/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/quynd/myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/quynd/myenv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/quynd/myenv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/quynd/myenv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.3 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
