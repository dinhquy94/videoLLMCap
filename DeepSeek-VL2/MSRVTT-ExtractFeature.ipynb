{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5b08e-1a92-4c84-8817-1edcb5693e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a18da63-3da2-4489-bc7d-0e7eb98f5c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/quynd/vidCapLLM/venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf34025-f3f8-4916-918c-48545ac72f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_clip_features(image_list, model):\n",
    "    \"\"\"\n",
    "    Trích xuất đặc trưng CLIP từ danh sách ảnh có kích thước khác nhau.\n",
    "\n",
    "    Args:\n",
    "        image_list (List[torch.Tensor]): Danh sách các tensor ảnh với kích thước khác nhau (C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor (N, 512) chứa vector đặc trưng của mỗi ảnh.\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    # Chuẩn hóa từng ảnh trong danh sách\n",
    "    image_batch = torch.stack([transform(img) for img in image_list])  # (N, 3, 224, 224)\n",
    "\n",
    "    # Trích xuất đặc trưng\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_batch)\n",
    "\n",
    "    # Chuẩn hóa vector đặc trưng\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    return image_features  # (N, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef440133-8a62-40b5-989f-fc698b1ebc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Features Shape: torch.Size([15, 512])\n",
      "Object Features Shape: torch.Size([15, 512])\n",
      "Context Features Shape: torch.Size([15, 512])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models.video import r3d_18  # Pretrained 3D-CNN model\n",
    "from transformers import DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms as T\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def extract_frames(video_path, N, M):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    key_frame_indices = np.linspace(0, frame_count - 1, N, dtype=int)\n",
    "\n",
    "    key_frames = np.zeros((N, height, width, 3), dtype=np.uint8)\n",
    "    surrounding_frames = np.zeros((N, M, height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, idx in enumerate(key_frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, key_frame = cap.read()\n",
    "        if ret:\n",
    "            key_frames[i] = key_frame\n",
    "\n",
    "        start_idx = max(0, idx - M // 2)\n",
    "        end_idx = min(frame_count, idx + M // 2)\n",
    "\n",
    "        temp_frames = []\n",
    "        for j in range(start_idx, end_idx):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, j)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                temp_frames.append(frame)\n",
    "\n",
    "        while len(temp_frames) < M:\n",
    "            temp_frames.append(temp_frames[-1] if temp_frames else key_frame)\n",
    "\n",
    "        surrounding_frames[i] = np.array(temp_frames[:M])\n",
    "\n",
    "    cap.release()\n",
    "    return key_frames, surrounding_frames\n",
    "\n",
    "def extract_context_vector(key_frames, clip_model):\n",
    "  transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "  return extract_clip_features([transform(frame) for frame in key_frames], clip_model)\n",
    "\n",
    "def extract_temporal_features(surrounding_frames, model):\n",
    "    N, M, h, w, c = surrounding_frames.shape\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),  # Resize frames to match model input\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    frames_tensor = torch.stack([transform(Image.fromarray(frame)) for frame in surrounding_frames.reshape(-1, h, w, c)])\n",
    "    frames_tensor = frames_tensor.view(N, M, 3, 112, 112).permute(0, 2, 1, 3, 4)  # (N, 3, M, 112, 112)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(frames_tensor)  # (N, feature_dim)\n",
    "    return features\n",
    "\n",
    "def detect_objects_with_detr(key_frames, detr_model, feature_extractor, clip_model):\n",
    "    N, h, w, c = key_frames.shape\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    key_features = []\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for i in range(N):\n",
    "        frame_tensor = transform(key_frames[i]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            outputs = detr_model(frame_tensor)\n",
    "\n",
    "        obj_features = []\n",
    "        scores = outputs.logits.softmax(-1)[0, :, :-1].max(-1)[0]\n",
    "        top_probs, top_labels = scores.max(-1)\n",
    "\n",
    "\n",
    "        probabilities = outputs.logits.softmax(-1)[0, :, :-1]  # Bỏ lớp background\n",
    "        top_probs, top_labels = probabilities.max(-1)\n",
    "\n",
    "        keep = top_probs > 0.7\n",
    "        labels = []\n",
    "        for label, score in zip(top_labels[keep], top_probs[keep]):\n",
    "          labels.append(id2label[label.item()])\n",
    "\n",
    "        keep = scores > 0.9  # Confidence threshold\n",
    "\n",
    "        boxes = outputs.pred_boxes[0, keep]\n",
    "        # print(labels)\n",
    "        object_list = []\n",
    "        for box in boxes:\n",
    "          x, y, w_box, h_box = box.numpy()\n",
    "          x_min = int((x - w_box / 2) * w)\n",
    "          y_min = int((y - h_box / 2) * h)\n",
    "          x_max = int((x + w_box / 2) * w)\n",
    "          y_max = int((y + h_box / 2) * h)\n",
    "\n",
    "          cropped_obj = key_frames[i][y_min:y_max, x_min:x_max]\n",
    "          if cropped_obj.shape[0] >= 7 and cropped_obj.shape[1] >= 7:  # Đảm bảo đủ lớn\n",
    "\n",
    "              obj_tensor = transform(cropped_obj)\n",
    "              object_list.append(obj_tensor)\n",
    "          else:\n",
    "              continue\n",
    "\n",
    "        if object_list:\n",
    "          with torch.no_grad():\n",
    "                obj_feature = extract_clip_features(object_list, clip_model).mean(dim=0)\n",
    "          key_features.append(obj_feature.squeeze().cpu().numpy())\n",
    "        else:\n",
    "          key_features.append(np.zeros((512,)))\n",
    "\n",
    "    return torch.tensor(key_features)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "video_path = \"./videoplayback.mp4\"\n",
    "N = 15  # Number of key frames\n",
    "M = 15  # Number of surrounding frames per key frame\n",
    "\n",
    "# Load models\n",
    "cnn3d = r3d_18(pretrained=True).eval()  # Pretrained 3D-CNN model\n",
    "cnn3d.fc = torch.nn.Identity()  # Remove classification layer\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").eval()\n",
    "feature_extractor = models.resnet50(pretrained=True)\n",
    "feature_extractor.fc = torch.nn.Identity()  # Remove classification layer\n",
    "id2label = detr_model.config.id2label\n",
    "\n",
    "model_name = 'ViT-B/32'  # Đảm bảo dùng đúng mô hình\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
    "\n",
    "\n",
    "key_frames, surrounding_frames = extract_frames(video_path, N, M)\n",
    "temporal_features = extract_temporal_features(surrounding_frames, cnn3d)\n",
    "object_features = detect_objects_with_detr(key_frames, detr_model, feature_extractor, clip_model)\n",
    "\n",
    "context_features = extract_context_vector(key_frames, clip_model)\n",
    "\n",
    "print(\"Motion Features Shape:\", temporal_features.shape)  # (N, feature_dim)\n",
    "print(\"Object Features Shape:\", object_features.shape)  # (N, feature_dim)\n",
    "print(\"Context Features Shape:\", context_features.shape)  # (N, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de9f7c9-64c0-4bd5-b2b7-072920f6a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-12 22:43:53--  https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip\n",
      "129.67.94.2ww.robots.ox.ac.uk (www.robots.ox.ac.uk)... \n",
      "connected. to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6552768292 (6.1G) [application/zip]\n",
      "Saving to: ‘MSRVTT.zip’\n",
      "\n",
      "MSRVTT.zip          100%[===================>]   6.10G  5.95MB/s    in 23m 1s  \n",
      "\n",
      "2025-03-12 23:06:55 (4.53 MB/s) - ‘MSRVTT.zip’ saved [6552768292/6552768292]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fa58b-849b-46ae-83be-897e94858d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./MSRVTT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f647eea2-172c-4e1d-956c-6f5af95e09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Load MSRVTT captions from a JSON file (assuming it's in a dictionary format)\n",
    "# Replace 'msrvtt_captions.json' with the actual path to your file\n",
    "with open('./MSRVTT/annotation/MSR_VTT.json', 'r') as file:\n",
    "    msrvtt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d59d6-9977-4773-86f4-45581c9a9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "msrvtt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f8888d2-700a-4f81-87a1-146633d04656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 199994/199994 [46:01<00:00, 72.42video/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm để hiển thị progress bar\n",
    "\n",
    "N = 15\n",
    "M = 16\n",
    "output_dir = \"video_features\"  # Thư mục để lưu file\n",
    "\n",
    "# Tạo thư mục nếu chưa tồn tại\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Sử dụng tqdm để hiển thị tiến trình\n",
    "for annotation in tqdm(msrvtt_data['annotations'], desc=\"Processing videos\", unit=\"video\"):\n",
    "    video_id = annotation['image_id']\n",
    "    output_file = os.path.join(output_dir, f\"{video_id}.pt\")\n",
    "\n",
    "    # Nếu file đã tồn tại, bỏ qua\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "\n",
    "    video_path = f\"./MSRVTT/videos/all/{video_id}.mp4\"\n",
    "    \n",
    "    key_frames, surrounding_frames = extract_frames(video_path, N, M)\n",
    "    temporal_features = extract_temporal_features(surrounding_frames, cnn3d)\n",
    "    object_features = detect_objects_with_detr(key_frames, detr_model, feature_extractor, clip_model) \n",
    "    context_features = extract_context_vector(key_frames, clip_model)\n",
    "\n",
    "    # Lưu tuple đặc trưng vào file riêng biệt\n",
    "    torch.save((temporal_features, object_features, context_features), output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f7d7f-64bf-4023-bbea-bee988b71004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
